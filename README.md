<h1 align="center"> üíª YouTube Comments Clustering üëæ </h1>
<p align="center"> An NLP project to cluster YouTube comments on the basis of their similarity of words </p>

## üìú Description

An [NLP](https://en.wikipedia.org/wiki/Natural_language_processing) Project in **Python3** that clusters YouTube comments made on a particular video into distinct groups on the basis of their similarity of words, and visualises the results using wordclouds and a bar graph plot; primarily using techniques like [k-Means clustering](https://en.wikipedia.org/wiki/K-means_clustering) and the [tf-idf](https://en.wikipedia.org/wiki/Tf%E2%80%93idf).


<br>

<div style="text-align: center;">
  <table style="margin: auto;">
    <tr>
      <td rowspan="2" align="center">
        <img src="https://github.com/user-attachments/assets/ac38ccf4-2c3c-4286-b539-84cd77375631" alt="Image 1" width="200">
      </td>
      <td align="center">
        <img src="https://github.com/user-attachments/assets/8b7dcfc9-4453-4cc8-a8d8-a36810571880" alt="Image 2" width="200">
      </td>
      <td align="center">
        <img src="https://github.com/user-attachments/assets/32b82f05-04a9-4d9a-ba97-500a786745fd" alt="Image 3" width="200">
      </td>
    </tr>
    <tr>
      <td align="center">
        <img src="https://github.com/user-attachments/assets/6544df17-e8c1-4065-ac48-6e10015b4fed" alt="Image 4" width="200">
      </td>
      <td align="center">
        <img src="https://github.com/user-attachments/assets/56a9a539-fbcb-40b1-b36e-9e8e210c449b" alt="Image 5" width="200">
      </td>
    </tr>
  </table>
</div>


<img src="https://github.com/user-attachments/assets/270b7a07-fd22-4209-a4b3-5ee40ce86266">


> Sample word clouds and bar graph plot to analyse the clustered comments' data; comments from this [video](https://youtu.be/IUTGFQpKaPU?si=pTZMHHYwLmggecWe)

<br>
<hr>

### The "Why" of the project
[This video](https://youtu.be/a-AqvPtjjts?si=jhjXuKKShjwqg_gb) whipped up the inspiration within me to create something like this, sometime in the future. And who knew this was the best time to begin fulfulling this long held longing!

Pondering for a few days had hit me up with this idea to cluster YouTube comments. 

Asked Why? :thinking:
- Firstly it could help us identify the genre of comments that were made the most on a particular video, and
- Secondly how many people resonated with them (i.e. which kind of comments were liked the most)

A simple yet an effective way to analyse people's reviews and opinions on a particular video. 
Sounds fair and square?

<br>
<hr>

### Usage
Click [here](https://github.com/TERNION-1121/YT-Comments-Clustering/blob/main/USAGE.md) to navigate to the `USAGE.md` file and go through the steps to make use of this project by yourself!


<br>
<hr>

### üéØ Learnings
This was my first NLP project, that too in Python! 

It was a nice experience learning about the basics of _What NLP is_, _the NLP pipeline_, _Text pre-processing and representation_, and to use these concepts in actual code.

One of the resources (in Hindi) I found really helpful was this [YouTube playlist](https://youtube.com/playlist?list=PLKnIA16_RmvZo7fp5kkIth6nRTeQQsjfX&si=a96yQTCTpoyOLMWO), these videos were really insightful and helped me understand my requirements and plan of action along the making of this project.

Not only did I get familiarized with the basics of `pandas`, but a part of this project also focused majorly on how to fetch the YouTube comments using the Google API. Trying to code that, along with a couple of documentations, references and resources available online, turned out to be a profound adventure on it's own.

<br>
<hr>

### ‚úèÔ∏è On Contributions
I have tried what I could to structure the code nicely; had also spent considerable time to speed up the text-preprocessing times. However, if one could help out with a better code or overall project organisation, or more optimised methods in various parts of the project, that would be highly appreciated!

Even README contributions would be of profound help!

<br>

I hope you found this project, and it's explanation valuable. Let me know about anything that could be made better. 
Thanks for your time!
